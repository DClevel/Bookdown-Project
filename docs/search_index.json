[["index.html", "ESS 580 - Intro to Data Science, Final Project Chapter 1 Introduction", " ESS 580 - Intro to Data Science, Final Project Daniel Cleveland 2022-04-01 Chapter 1 Introduction This book compiles six of seven different assignments given in the Spring 2022 semester Introduction to Environmental Data Science (ESS 580) course offered at Colorado State University. "],["poudre-river-rmarkdown-and-github-familiarization.html", "Chapter 2 Poudre River: RMarkdown and GitHub Familiarization 2.1 Methods 2.2 Results 2.3 Assignment", " Chapter 2 Poudre River: RMarkdown and GitHub Familiarization Assignment 1: Learning to Create GitHub Website from an RMarkdown file. 2.1 Methods (Learning Simple Formatting) The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban stormwater Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultral diversions 2.1.1 SiteDescription (How to insert web links) A link was given to insert the below image found online: 2.1.2 Data Acquisition and Plotting tests (Unused section) 2.1.3 Data Download (How to hide code from output) 2.1.4 Static Data Plotter (Creating First Plot) 2.1.5 Interactive Data Plotter *(Creating Interactive Plots) (Creating First Interactive Plot) 2.2 Results (Unused Section) 2.3 Assignment This assignment will be primarily about demonstrating some expertice in using RMarkdown, since we will be using Rmds as the primary form of homework and assignments. With that in mind, your assignment for this homework is to: Fork the example repository into your personal GitHub Create an RStudio project from your Personal clone of the Repo. Create a table of contents that is floating, but displays three levels of headers instead of two (by editing the content at the beginning of the document) Make a version of the dygraph with points and lines by using rstudios dygraph guide Writing a paragraph on the Poudre river with at least three hyperlinks, two bolded sections, and one italicized phrase. The content of this paragraph is not vital, but try to at least make it true and interesting, and, of course, dont plagiarize. Knit that document, and then git commit and push to your personal GitHub. Use the GitHub -&gt; Settings -&gt; Pages tab to create a website of your report. Bonus, make the timestamp in the header dynamic. As in it only adds todays date, not just a static date you enter. Bonus, create an index_talk.Rmd version of your document using the revealjs package. Add link to your original report-style document. 2.3.1 DyGraph example. (Practicing plotting) dygraph(q_xts) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;)%&gt;% dyOptions(drawPoints = TRUE, pointSize = 2) 2.3.2 Poudre Paragraph (Practicing Formatting and inserting links) According to VisitFortCollins.com The Poudre (pronounced pooh-der) is Colorados only nationally designated Wild &amp; Scenic River. The Poudre has gained recognition of late because 2018 marked the 50th anniversary of the Wild &amp; Scenic Rivers Systems. The Cache la Poudre National Heritage Area was established in 1983. There are three national Heritage Areas in Colorado, including the Cache La Poudre River Natural Heritage Area. There are a number of ways to explore the area, including walking and biking tours near parks, lakes, the Poudre River Trail and numerous historical sites. From the National Park Service page on Cache la Poudre River National Heritage Area, we learn the following. The Cache la Poudre River National Heritage Area (CALA), designated by Congress in 2009, is one of 55 National Heritage Areas throughout the United States and was the first to be established west of the Mississippi. The CALA extends for 45 miles and includes the lands within the 100-year flood plain of the Cache la Poudre River. It begins in Larimer County at the eastern edge of the Roosevelt National Forest and ends in Weld County, at the confluence of the South Platte River just east of Greeley. The Forest Service provides this map of the Poudre River, on their Arapaho &amp; Roosevelt National Forests Pawnee National Grassland webpage. "],["hayman-fire-recovery.html", "Chapter 3 Hayman Fire Recovery 3.1 Reading In and Preparing Fire Data 3.2 The Assignment", " Chapter 3 Hayman Fire Recovery Assignment 2: Now that we have learned how to munge (manipulate) data and plot it, we will work on using these skills in new ways 3.1 Reading In and Preparing Fire Data ####-----Reading in Data and Stacking it ----- #### #Reading in files files &lt;- list.files(&#39;data_fire&#39;,full.names=T) #Read in individual data files ndmi &lt;- read_csv(files[1]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndmi&#39;) ndsi &lt;- read_csv(files[2]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;) ndvi &lt;- read_csv(files[3])%&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;) # Stack as a tidy dataset full_long &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% gather(key=&#39;treatment&#39;,value=&#39;veg_index&#39;,-DateTime,-data) %&gt;% filter(!is.na(veg_index)) 3.2 The Assignment 3.2.1 Question 1) What is the correlation between NDVI and NDMI? - here I want you to convert the full_long dataset in to a wide dataset using the function spread and then make a plot that shows the correlation s a function of if the site was burned or not (x axis should be ndmi) You should exclude winter months and focus on summer months # preparing ndvi/ndmi/ndsi dataset for plotting full_wide &lt;- spread(data=full_long,key=&#39;data&#39;,value=&#39;veg_index&#39;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) summer_only &lt;- filter(full_wide,month %in% c(6,7,8,9)) # plotting the data to exmaine the NDVI vs NDMI ggplot(summer_only,aes(x=ndmi,y=ndvi,color=treatment)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE)+ stat_cor()+ ylim(0, 0.7)+ theme_few() + scale_color_colorblind() + theme(legend.position=c(0.9,0.85)) As can be seen in the above plot, an increase in NDMI is generally associated with an increase in NDVI. While this relationship is apparent for both the unburned and burned treatments, it is more clearly seen in the data from the burned treatment. The R value for the burned treatment (R = 0.71) is greater than that of the unburned treatment (R = 0.17). 3.2.2 Question 2 What is the correlation between average NDSI (normalized snow index) for January - April and average NDVI for June-August? In other words, does the previous years snow cover influence vegetation growth for the following summer? For this question, the student showed a way the data could be reorganized without using group_by and instead simply using gather and spread ###getting annual averages for NDSI by year for unburned and burned ndsi2 &lt;- read_csv(files[2]) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;, month = month(DateTime))%&gt;% filter(month %in% c(1,2,3,4)) ndsi2_burn &lt;- select(ndsi2, 1:2)%&gt;% mutate(ndsi_burn = &#39;ndsi_burn&#39;, year = year(DateTime))%&gt;% filter(!is.na(burned)) ndsi2_burn_wide &lt;- spread(ndsi2_burn,key = &#39;year&#39;, value = &#39;burned&#39;) ndsi2burnAve &lt;- summarise_if(ndsi2_burn_wide,is.numeric,mean, na.rm = TRUE)%&gt;% mutate(trt = &#39;NDSI_burnedAve&#39;)%&gt;% relocate(trt) ndsi2_unburn &lt;- select(ndsi2, 1,3)%&gt;% mutate(ndsi_unburn = &#39;ndsi_unburn&#39;, year = year(DateTime))%&gt;% filter(!is.na(unburned)) ndsi2_unburn_wide &lt;- spread(ndsi2_unburn,key = &#39;year&#39;, value = &#39;unburned&#39;) ndsi2unburnAve &lt;- summarise_if(ndsi2_unburn_wide,is.numeric,mean, na.rm = TRUE)%&gt;% mutate(trt = &#39;NDSI_unburnedAve&#39;)%&gt;% relocate(trt) ###getting annual averages for NDVI by year for unburned and burned ndvi2 &lt;- read_csv(files[3])%&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;, month = month(DateTime))%&gt;% filter(month %in% c(6,7,8)) ndvi2_burn &lt;- select(ndvi2, 1:2)%&gt;% mutate(ndvi_burn = &#39;ndvi_burn&#39;, year = year(DateTime))%&gt;% filter(!is.na(burned)) ndvi2_burn_wide &lt;- spread(ndvi2_burn,key = &#39;year&#39;, value = &#39;burned&#39;) ndvi2burnAve &lt;- summarise_if(ndvi2_burn_wide,is.numeric,mean, na.rm = TRUE)%&gt;% mutate(trt= &#39;NDVI_burnedAve&#39;)%&gt;% relocate(trt) ndvi2_unburn &lt;- select(ndvi2, 1,3)%&gt;% mutate(ndvi_unburn = &#39;ndvi_unburn&#39;, year = year(DateTime))%&gt;% filter(!is.na(unburned)) ndvi2_unburn_wide &lt;- spread(ndvi2_unburn,key = &#39;year&#39;, value = &#39;unburned&#39;) ndvi2unburnAve &lt;- summarise_if(ndvi2_unburn_wide,is.numeric,mean, na.rm = TRUE)%&gt;% mutate(trt = &#39;NDVI_unburnedAve&#39;)%&gt;% relocate(trt) ## reorganizing data for in preparation for plotting bind &lt;- rbind(ndsi2burnAve,ndsi2unburnAve,ndvi2burnAve,ndvi2unburnAve) bindlong &lt;- gather(bind, key = &#39;year&#39;, value = &#39;index&#39;, -trt) trtspread &lt;- spread(bindlong, key = &#39;trt&#39;, value = &#39;index&#39;) NDSI2 &lt;- select(trtspread, 1:3)%&gt;% rename(burned = 2, unburned = 3)%&gt;% mutate(trt = &#39;NDSI2&#39;)%&gt;% gather(key = &#39;burn_stat&#39;, value = &#39;index&#39;, -year,-trt) NDVI2 &lt;- select(trtspread,1,4,5)%&gt;% rename(burned =2, unburned = 3)%&gt;% mutate(trt = &#39;NDVI2&#39;)%&gt;% gather(key = &#39;burn_stat&#39;, value = &#39;index&#39;, -year,-trt) bind_again &lt;- rbind(NDSI2, NDVI2) lastspread &lt;- spread(bind_again, key = &#39;trt&#39;, value = &#39;index&#39;) ## plotting the data ggplot(lastspread,aes(x=NDSI2,y=NDVI2,color=burn_stat)) + geom_point() + geom_smooth(method = lm) + stat_cor() + theme_few() + scale_color_colorblind(name= &quot;Treatment&quot;) + theme(legend.position=c(0.9,0.15)) + labs(x = &quot;NDSI&quot;, y = &quot;NDVI&quot;, title = &quot;Average Summer NDVI (Jun-Aug) vs. Average Winter NDSI (Jan-Apr)&quot;) Based on the above plot, there does not appear to be a strong relationship between snowfall over the winter and vegetation growth the following summer, especially for the unburned treatment. Based on the above plot alone, we might at first conclude that any association between an increase in winter snowfall and an increase in vegetation is higher for the burned treatment. However, this would likely be a mistake, as this data contains both pre-burn and post-burn data, and the average NDVI values dramatically dropped after the burn. That drop can be seen in the above data, with point above the burned regression line likely representing pre-burn, and those below likely representing post-burn. Without that drop in NDVI due to the burn treatment, or taking each treatment separately, we would likely see that the burned plot region has a similarly low, even negative, R value as the unburned plot. We will examine this more closely in the following figure. 3.2.3 Q3 How is the snow effect from question 2 different between pre- and post-burn and burned and unburned? Students interpretation of this question: For the plot that received the burn treatment, how does the snow effect vary from pre-burn to post-burn. Further, how does this variation compare to the effect seen between the burned and unburned plots in the question 2 analysis. ## Reading, Filtering, Splitting Burned Plot Data into pre-burn and post-burn ndvi3_preburn &lt;- read_csv(files[3])%&gt;% select(1,2)%&gt;% rename(value =2)%&gt;% mutate(month = month(DateTime), year = year(DateTime))%&gt;% filter(DateTime &gt;= as.Date(&quot;1984-01-01&quot;), DateTime &lt;= as.Date(&quot;2002-06-07&quot;), month %in% c(6,7,8), !is.na(value))%&gt;% group_by(year)%&gt;% summarise(avg = mean(value))%&gt;% mutate(data_type = &quot;NDVI&quot;, time_period = &quot;preburn&quot;) ndvi3_postburn &lt;- read_csv(files[3])%&gt;% select(1,2)%&gt;% rename(value =2)%&gt;% mutate(month = month(DateTime), year = year(DateTime))%&gt;% filter(DateTime &gt;= as.Date(&quot;2002-07-19&quot;), month %in% c(6,7,8), !is.na(value))%&gt;% group_by(year)%&gt;% summarise(avg = mean(value))%&gt;% mutate(data_type = &quot;NDVI&quot;, time_period = &quot;postburn&quot;) ndsi3_preburn &lt;- read_csv(files[2])%&gt;% select(1,2)%&gt;% rename(value =2)%&gt;% mutate(month = month(DateTime), year = year(DateTime))%&gt;% filter(DateTime &gt;= as.Date(&quot;1984-01-01&quot;), DateTime &lt;= as.Date(&quot;2002-06-07&quot;), month %in% c(1,2,3,4), !is.na(value))%&gt;% group_by(year)%&gt;% summarise(avg = mean(value))%&gt;% mutate(data_type = &quot;NDSI&quot;, time_period = &quot;preburn&quot;) ndsi3_postburn &lt;- read_csv(files[2])%&gt;% select(1,2)%&gt;% rename(value =2)%&gt;% mutate(month = month(DateTime), year = year(DateTime))%&gt;% filter(DateTime &gt;= as.Date(&quot;2002-07-19&quot;), month %in% c(1,2,3,4), !is.na(value))%&gt;% group_by(year)%&gt;% summarise(avg = mean(value))%&gt;% mutate(data_type = &quot;NDSI&quot;, time_period = &quot;postburn&quot;) ## Stacking and then Spreading Data for plotting preNpost &lt;- rbind(ndvi3_preburn, ndvi3_postburn, ndsi3_preburn, ndsi3_postburn)%&gt;% spread(key = &#39;data_type&#39;, value = &#39;avg&#39;)%&gt;% filter_if(is.numeric,all_vars(!is.na(.))) ## plotting the data ggplot(preNpost, aes(x = NDSI, y = NDVI, color = time_period)) + geom_point()+ geom_smooth(method = lm)+ stat_cor()+ theme_few()+ scale_color_colorblind(name= &quot;Time Period&quot;)+ theme(legend.position = c(0.9, 0.15)) + labs(title = &quot;How NDVI-NDSI Relationship Changed After Hayman Fire&quot;) In analyzing the relationship between average winter snowfall and average summer vegetation growth explicitly for the burned plot, we do not see a strong association. Trendlines for both pre-fire and post-fire data show have slopes near zero. However, we are able to see a clear drop in average summer NDVI values after the fire. While the association between NDVI and NDSI is most likely slightly positive for the pre-burn time period, the relationship looks to most likely have become slightly negative after the fire. 3.2.4 Question 4 What month is the greenest month on average? ## preparing the data ndvi4 &lt;- read_csv(files[3])%&gt;% rename(burned = 2, unburned = 3)%&gt;% gather(key = &#39;treatment&#39;, value = &#39;veg_index&#39;, -DateTime)%&gt;% filter(!is.na(veg_index))%&gt;% mutate(month = month(DateTime))%&gt;% mutate(Month = month.abb[month]) ndviBAR4b &lt;- ndvi4 %&gt;% filter(treatment == &quot;burned&quot;)%&gt;% group_by(month)%&gt;% summarise(AVG = mean(veg_index))%&gt;% mutate(Month = month.abb[month], treatment = &#39;burned&#39;) ndviBAR4u &lt;- ndvi4 %&gt;% filter(treatment == &quot;unburned&quot;)%&gt;% group_by(month)%&gt;% summarise(AVG = mean(veg_index))%&gt;% mutate(Month = month.abb[month], treatment = &#39;unburned&#39;) ndviBAR4 &lt;- rbind(ndviBAR4b, ndviBAR4u) topNDVI4 &lt;- ndvi4 %&gt;% filter(month %in% c(6,7,8,9,10)) ## plotting the data ggplot(ndviBAR4, aes(x=reorder(Month,AVG), y=AVG)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;darkgreen&quot;)+ theme_minimal()+ facet_wrap(~treatment)+ labs(x = &quot;&quot;, y = &quot;Average NDVI&quot;, title = &quot;Average NDVI by Month for Each Treatment&quot;) ggplot(topNDVI4, aes(x = reorder(Month,veg_index), y= veg_index)) + geom_boxplot() + geom_jitter(width = 0.1, alpha = 0.2)+ stat_summary(fun.y = &quot;mean&quot;, color = &quot;limegreen&quot;) + scale_fill_viridis(discrete = TRUE, alpha=0.6, option=&quot;A&quot;) + theme_minimal() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=11) ) + ggtitle(&quot;NDVI Distributions for 5 Greenest Months by Treatment&quot;) + ylab(&quot;NDVI&quot;) + xlab(&quot;&quot;)+ facet_wrap(~treatment) The above bar chart shows how average monthly NDVI values differ between the burned and unburned plots. September and August are on average the two greenest months for both plots, but September is slightly greener on the unburned plot, while August is slightly greener on the burned plot. Taking a closer a look at the distribution of the data, which can be seen in the above boxplots, a clear break can be seen in the burned plots data. This break most certainly corresponds to the drop in NDVI that occurred after the fire. Therefore, to properly analyze which month is greenest in the burned plot, it is best to analyze its greenest after the fire separately from greenness before the fire. 3.2.5 Question 5) What month is the snowiest on average? ## preparing the data ndsi5 &lt;- read_csv(files[2])%&gt;% rename(burned = 2, unburned = 3)%&gt;% gather(key = &#39;treatment&#39;, value = &#39;snow_index&#39;, -DateTime)%&gt;% filter(!is.na(snow_index))%&gt;% mutate(month = month(DateTime))%&gt;% mutate(Month = month.abb[month]) ndsiBAR5b &lt;- ndsi5 %&gt;% filter(treatment == &quot;burned&quot;)%&gt;% group_by(month)%&gt;% summarise(AVG = mean(snow_index))%&gt;% mutate(Month = month.abb[month], treatment = &#39;burned&#39;) ndsiBAR5u &lt;- ndsi5 %&gt;% filter(treatment == &quot;unburned&quot;)%&gt;% group_by(month)%&gt;% summarise(AVG = mean(snow_index))%&gt;% mutate(Month = month.abb[month], treatment = &#39;unburned&#39;) ndsiBAR5 &lt;- rbind(ndsiBAR5b, ndsiBAR5u) topNDSI5 &lt;- ndsi5 %&gt;% filter(month %in% c(1,2,3,11,12)) ## plotting the data ggplot(ndsiBAR5, aes(x=reorder(Month,AVG), y=AVG)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;blue&quot;)+ theme_minimal()+ facet_wrap(~treatment)+ labs(x = &quot;&quot;, y = &quot;Average NDSI&quot;, title = &quot;Average NDSI by Month for Each Treatment&quot;) ggplot(topNDSI5, aes(x = reorder(Month,snow_index), y= snow_index)) + geom_boxplot() + geom_jitter(width = 0.1, alpha = 0.2)+ stat_summary(fun.y = &quot;mean&quot;, color = &quot;blue&quot;) + scale_fill_viridis(discrete = TRUE, alpha=0.6, option=&quot;A&quot;) + theme_minimal() + theme( legend.position=&quot;none&quot;, plot.title = element_text(size=11) ) + ggtitle(&quot;NDSI Distribution for 5 Snowiest Months by Treatment&quot;) + ylab(&quot;NDSI&quot;) + xlab(&quot;&quot;)+ facet_wrap(~treatment) After organizing the data for each region according to month and plotting monthly NDSI distributions and mean values in the above boxplots, we can see that the snowiest months differ between regions. January is the snowiest month on average in the burned region, whereas February is the snowiest month on average in the unburned region. Again, taking a closer a look at the distribution of the data, which can be seen in the above boxplots, a clear break can be seen in the burned plots data, especially for the snowiest months of December, January, and February. This break likely also corresponds to the vegetation loss and ecosystem change that occurred after the fire. Therefore, to properly analyze which month is snowiest in the burned plot, it is best to analyze its snow index after the fire separately from before the fire. 3.2.6 Bonus Question 1: Redo all problems with spread and gather using modern tidyverse syntax. 3.2.7 Bonus Question 2: Use Climage Engine to pull the same data for the assignment, but updated with 2020/2021 data. "],["snow-data.html", "Chapter 4 Snow Data 4.1 Simple web scraping 4.2 Assignment:", " Chapter 4 Snow Data Assignment 3: Using some simple web-scraping to find links to data files, download data, and analyze it. 4.1 Simple web scraping R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalance Studies Website and read a table in. This table contains links to data we want to programatically download for three sites. We dont know much about these sites, but they contain incredibly rich snow, temperature, and precip data. 4.1.1 Reading an html 4.1.1.1 Extract CSV links from webpage site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #See if we can extract tables and get the data that way: # tables &lt;- webpage %&gt;% # html_nodes(&#39;table&#39;) %&gt;% # magrittr::extract2(3) %&gt;% # html_table(fill = TRUE) #That didn&#39;t work, so let&#39;s try a different approach #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 4.1.2 Data Download 4.1.2.1 Download data in a for loop setwd(&quot;C:/Users/DClev/Documents/ESS580_copied_folder/Bookdown_Proj/Bookdown4/Bookdown-Project&quot;) #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes file_names &lt;- paste0(&#39;data_snow/&#39;,dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) # confirming that NOT all files were downloaded: evaluate &lt;- !all(downloaded) 4.1.2.2 Download data in a map setwd(&quot;C:/Users/DClev/Documents/ESS580_copied_folder/Bookdown_Proj/Bookdown4/Bookdown-Project&quot;) #Map version of the same for loop (downloading 3 files) if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 0 4.1.3 Data read-in 4.1.3.1 Read in just the snow data as a loop #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] snow_files ## [1] &quot;data_snow/SASP_24hr.csv&quot; &quot;data_snow/SBSP_24hr.csv&quot; #empty_data &lt;- list() # snow_data &lt;- for(i in 1:length(snow_files)){ # empty_data[[i]] &lt;- read_csv(snow_files[i]) %&gt;% # select(Year,DOY,Sno_Height_M) # } #snow_data_full &lt;- do.call(&#39;rbind&#39;,empty_data) #summary(snow_data_full) 4.1.3.2 Read in the data as a map function our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) summary(snow_data_full) ## Year DOY Sno_Height_M site ## Min. :2003 Min. : 1.0 Min. :-3.523 Length:12786 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 Class :character ## Median :2012 Median :183.0 Median : 0.978 Mode :character ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 4.1.4 Plot snow data snow_yearly &lt;- snow_data_full %&gt;% group_by(Year,site) %&gt;% summarize(mean_height = mean(Sno_Height_M,na.rm=T)) ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + geom_point() + ylab(&#39;Mean Snow Height (meters)&#39;)+ ggthemes::theme_few() + ggthemes::scale_color_few() 4.2 Assignment: 4.2.0.1 Q1: Extract Data Extract the meteorological data URLs. Here we want you to use the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological datasets. site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;forcing&#39;,.)] %&gt;% html_attr(&#39;href&#39;) links ## [1] &quot;https://snowstudies.org/wp-content/uploads/2022/02/SBB_SASP_Forcing_Data.txt&quot; ## [2] &quot;https://snowstudies.org/wp-content/uploads/2022/02/SBB_SBSP_Forcing_Data.txt&quot; 4.2.0.2 Q2: Download Data Download the meteorological data. Use the download_file and str_split_fixed commands to download the data and save it in your data folder. You can use a for loop or a map function. setwd(&quot;C:/Users/DClev/Documents/ESS580_copied_folder/Bookdown_Proj/Bookdown4/Bookdown-Project&quot;) #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes m_file_names &lt;- paste0(&#39;data_snow/&#39;,dataset) for(i in 1:2){ download.file(links[i],destfile=m_file_names[i]) } downloaded &lt;- file.exists(m_file_names) # confirming that NOT all files were downloaded: evaluate &lt;- !all(downloaded) m_file_names ## [1] &quot;data_snow/SBB_SASP_Forcing_Data.txt&quot; &quot;data_snow/SBB_SBSP_Forcing_Data.txt&quot; 4.2.0.3 Q3: Write Read-In Function Write a custom function to read in the data and append a site column to the data. # this code grabs the variable names from the metadata pdf file library(pdftools) headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:26] %&gt;% str_trim(side = &quot;left&quot;) # Writing Read-in Function our_meteor_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_Forcing_Data.txt&#39;,&#39;&#39;,.) df &lt;- read_table(file, col_names = headers) %&gt;% select(c(1,2,10))%&gt;% mutate(site = name)%&gt;% filter_if(is.numeric,all_vars(!is.na(.))) } 4.2.0.4 Q4: Use Map Use the map function to read in both meteorological files. Display a summary of your tibble. meteor_data_full &lt;- map_dfr(m_file_names,our_meteor_reader) summary(meteor_data_full) ## year month air temp [K] site ## Min. :2003 Min. : 1.000 Min. :242.1 Length:94467 ## 1st Qu.:2005 1st Qu.: 3.000 1st Qu.:266.0 Class :character ## Median :2006 Median : 6.000 Median :272.9 Mode :character ## Mean :2007 Mean : 6.446 Mean :272.9 ## 3rd Qu.:2009 3rd Qu.: 9.000 3rd Qu.:279.9 ## Max. :2011 Max. :12.000 Max. :295.8 4.2.0.5 Q5: Line Plot - Average Yearly Temp by Site Make a line plot of mean temp by year by site (using the air temp [K] variable). Is there anything suspicious in the plot? Adjust your filtering if needed. # Finding by mean temp by year by site: line_plot_data &lt;- meteor_data_full%&gt;% group_by(year,site)%&gt;% summarise(Mean_Air_Temp = mean(`air temp [K]`)) # Creating Line Plot ggplot(line_plot_data, aes(x = year, y = Mean_Air_Temp, color = site)) + geom_line() + ylab(&#39;Mean Air Temperature (Kelvin)&#39;)+ ggthemes::theme_few() + ggthemes::scale_color_few() We can see that data from 2003 looks suspicious. Looking at the data we can see that 2003 data only begins in November. As we dont have a full years worth of data, we should filter this data out from our analysis here. Also, looking at 2011, we see that the data ends on October 1st, and so 2011 should also be filtered out. However, further, it appears that data prior to 2005 is significantly different than after. The data before 2005 should be scrutinized. It may be best to also filter out all data prior to 2005. 4.2.0.5.1 Refiltering and Replotting: ann_line_plot_data &lt;- meteor_data_full%&gt;% filter(year &gt; 2003, year &lt;2011)%&gt;% group_by(year,site)%&gt;% summarise(Mean_Air_Temp = mean(`air temp [K]`)) # Creating Line Plot after refiltering ggplot(ann_line_plot_data, aes(x = year, y = Mean_Air_Temp, color = site)) + geom_line() + ylab(&#39;Mean Air Temperature (Kelvin)&#39;)+ ggthemes::theme_few() + ggthemes::scale_color_few() (It may be that data for SBB_SBSP is unreliable prior to 2005.) 4.2.0.6 Q6: Write Plotting Function Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot? Hint: https://ggplot2.tidyverse.org/reference/print.ggplot.html years &lt;- (2005:2010) plot_fun &lt;- function(year_input){ year_input &lt;- years[i] plot_fun_data &lt;- meteor_data_full%&gt;% group_by(year,month,site)%&gt;% summarise(monthly_ave_temp = mean(`air temp [K]`))%&gt;% filter(year == year_input) print(ggplot(plot_fun_data, aes(x = month, y = monthly_ave_temp, color = site)) + geom_line() + ggtitle(year_input) + ylab(&#39;Mean Air Temperature (Kelvin)&#39;)+ ggthemes::theme_few() + ggthemes::scale_color_few()) } for(i in 1:length(years)){ plot_fun(years[i]) } Bonus: Make a plot of average daily precipitation by day of year (averaged across all available years). Color each site. Bonus #2: Use a function and for loop to create yearly plots of precipitation by day of year. Color each site. "],["lagos-data-analysis.html", "Chapter 5 LAGOS Data Analysis 5.1 Setting Up for the Assignment 5.2 Assignment", " Chapter 5 LAGOS Data Analysis Assignment 4: Geospatial Analysis Part I Using the LAGOS dataset to learn how to work with spatial data. 5.1 Setting Up for the Assignment 5.1.1 Loading in data 5.1.1.1 First download and then specifically grab the locus (or site lat longs) # #Lagos download script LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus 5.1.1.2 Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% st_transform(2163) #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer mapview(subset_spatial, legend = TRUE, layer.name = &#39;First 100 LAGOS Lakes&#39;) 5.1.1.3 Subset to only Minnesota states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) #Subset lakes based on spatial position minnesota_lakes &lt;- spatial_lakes[minnesota,] #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;, legend = TRUE, layer.name = &#39;Lakes Area (hectares)&#39;) 5.2 Assignment 5.2.1 1) Show a map outline of Iowa and Illinois (similar to Minnesota map upstream) #Plotting Illinois and Iowa on same map: illinois_iowa &lt;- states %&gt;% arrange(name)%&gt;% filter(name %in% c(&#39;Illinois&#39;,&#39;Iowa&#39;))%&gt;% st_transform(2163) mapview(illinois_iowa, legend = TRUE, layer.name = &#39;Iowa and Illinois&#39;) 5.2.2 2) Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota? #Subset lakes based on spatial position illinois_iowa_lakes &lt;- spatial_lakes[illinois_iowa,] # ddply(illinois_iowa_lakes,.(lagoslakeid),nrow) str(illinois_iowa_lakes) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 16466 obs. of 17 variables: ## $ lagoslakeid : int 4280 4285 4292 4312 4314 4322 4351 4363 4389 4393 ... ## $ nhdid : chr &quot;155844663&quot; &quot;137027971&quot; &quot;132548442&quot; &quot;146615193&quot; ... ## $ gnis_name : chr &quot;Eldred Sherwood Park Lake&quot; &quot;Badger Lake&quot; NA NA ... ## $ lake_area_ha : num 8.59 17.27 3.97 10.68 208.45 ... ## $ lake_perim_meters: num 2747 4146 1603 2323 18841 ... ## $ nhd_fcode : int 39004 39004 39004 39004 39004 39004 39004 39009 39004 39004 ... ## $ nhd_ftype : int 390 390 390 390 390 390 390 390 390 390 ... ## $ iws_zoneid : chr &quot;IWS_31926&quot; &quot;IWS_32272&quot; NA &quot;IWS_3378&quot; ... ## $ hu4_zoneid : chr &quot;HU4_57&quot; &quot;HU4_56&quot; &quot;HU4_61&quot; &quot;HU4_56&quot; ... ## $ hu6_zoneid : chr &quot;HU6_78&quot; &quot;HU6_79&quot; &quot;HU6_87&quot; &quot;HU6_79&quot; ... ## $ hu8_zoneid : chr &quot;HU8_400&quot; &quot;HU8_403&quot; &quot;HU8_419&quot; &quot;HU8_406&quot; ... ## $ hu12_zoneid : chr &quot;HU12_9763&quot; &quot;HU12_3119&quot; &quot;HU12_2466&quot; &quot;HU12_3425&quot; ... ## $ edu_zoneid : chr &quot;EDU_23&quot; &quot;EDU_74&quot; &quot;EDU_4&quot; &quot;EDU_61&quot; ... ## $ county_zoneid : chr &quot;County_248&quot; &quot;County_204&quot; &quot;County_207&quot; &quot;County_251&quot; ... ## $ state_zoneid : chr &quot;State_13&quot; &quot;State_13&quot; &quot;State_13&quot; &quot;State_13&quot; ... ## $ elevation_m : num 372 328 340 287 266 ... ## $ geometry :sfc_POINT of length 16466; first list element: &#39;XY&#39; num 524845 -208157 ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;lagoslakeid&quot; &quot;nhdid&quot; &quot;gnis_name&quot; &quot;lake_area_ha&quot; ... There are 16,466 different sites in Illinois and Iowa combined. In Minnesota alone there are 29,038 sites. 5.2.3 3) What is the distribution of lake size in Iowa vs. Minnesota? Here I want to see a histogram plot with lake size on x-axis and frequency on y axis (check out geom_histogram) # What is the &quot;state_zoneid&quot; value for Iowa? ## (We already know that Minnesota&#39;s state_zoneid = State_14) ## Confirmed that Iowa = State_13 # iowa &lt;- states %&gt;% # arrange(name)%&gt;% # filter(name %in% c(&#39;Iowa&#39;))%&gt;% # st_transform(2163) # mapview(iowa) # # iowa_lakes &lt;- spatial_lakes[iowa,] # str(iowa_lakes) (Found that Iowa indeed has state_zoneid of State_13 in the spatial_lakes data.) # Getting Minnesota vs Iowa Data in one sf dataframe ## creating spatial filter boundary: minnesota_iowa &lt;- states %&gt;% arrange(name)%&gt;% filter(name %in% c(&#39;Minnesota&#39;,&#39;Iowa&#39;))%&gt;% st_transform(2163) mapview(minnesota_iowa, legend = TRUE, layer.name = &#39;Minnesota and Iowa&#39;) ## filtering data by area, creating state_name column, removing unneccesary data minnesota_iowa_lakes &lt;- spatial_lakes[minnesota_iowa,] %&gt;% mutate(state_name = case_when(state_zoneid == &quot;State_14&quot; ~ &quot;Minnesota&quot;, state_zoneid == &quot;State_13&quot; ~ &quot;Iowa&quot;, TRUE ~ &quot;error&quot;)) %&gt;% select(lake_area_ha,state_name)%&gt;% filter(!state_name == &quot;error&quot;) arrange(minnesota_iowa_lakes, state_name, lake_area_ha) ## Simple feature collection with 33611 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 203919.6 ymin: -480215.1 xmax: 813541.2 ymax: 493918.3 ## Projected CRS: NAD27 / US National Atlas Equal Area ## First 10 features: ## lake_area_ha state_name geometry ## 1 1.000077 Iowa POINT (742604 -379229.3) ## 2 1.000400 Iowa POINT (515669.8 -337658.9) ## 3 1.000417 Iowa POINT (507368.8 -450353.2) ## 4 1.000624 Iowa POINT (580891.2 -370334.6) ## 5 1.000978 Iowa POINT (692479.6 -410089.1) ## 6 1.000999 Iowa POINT (470411.5 -418019.3) ## 7 1.001508 Iowa POINT (476085.4 -409184) ## 8 1.001624 Iowa POINT (510446 -468205.6) ## 9 1.001692 Iowa POINT (367883 -472857) ## 10 1.001693 Iowa POINT (602137.5 -337188.5) # creating histogram: ggplot(minnesota_iowa_lakes, aes(x=log10(lake_area_ha), fill=state_name)) + geom_histogram( color=&quot;#e9ecef&quot;, alpha=0.6, position = &#39;identity&#39;) + xlab(&#39;log10(lake area (in hectares))&#39;)+ scale_fill_manual(values=c(&quot;#404080&quot;, &quot;#69b3a2&quot;)) + theme_ipsum() + labs(fill=&quot;&quot;)+ guides(col=guide_legend(&quot;State&quot;)) Comparing the distributions of lake size on the log10 scale for Iowa on top of Minnesota reveals that the both states have very similar lake area distributions. Minnesota has more lakes than Iowa, but the distribution of those lake by area is very comparable to Iowa. Minnesota also appears to have larger lakes than Iowa. 5.2.4 4) Make an interactive plot of lakes in Iowa and Illinois and color them by lake area in hectares # Creating interactive map for the largest 1000 lakes across Illinois and Iowa illinois_iowa_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;, legend = TRUE, layer.name = &#39;Lake Area (hectares)&#39;) 5.2.5 5) What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states? We could perhaps analyze some of the shapefiles that USGS provides for North American lakes: https://www.sciencebase.gov/catalog/item/4fb55df0e4b04cb937751e02 "],["lake-water-quality-analysis.html", "Chapter 6 Lake Water Quality Analysis 6.1 Preparing for LAGOS Analysis Assignment 6.2 Class work", " Chapter 6 Lake Water Quality Analysis Assignment 5: Geospatial Analysis Part II Learning to join spatial data with water quality data and analyze relationships 6.1 Preparing for LAGOS Analysis Assignment 6.1.1 Loading in data 6.1.1.1 First download and then specifically grab the locus (or site lat longs) #Lagos download script #lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus # Make an sf object spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) #Grab the water quality data nutr &lt;- lagos$epi_nutr #Look at column names #names(nutr) 6.1.1.2 Subset columns nutr to only keep key info that we want clarity_only &lt;- nutr %&gt;% select(lagoslakeid,sampledate,chla,doc,secchi) %&gt;% mutate(sampledate = as.character(sampledate) %&gt;% ymd(.)) 6.1.1.3 Keep sites with at least 200 observations #Look at the number of rows of dataset #nrow(clarity_only) chla_secchi &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi)) # How many observatiosn did we lose? # nrow(clarity_only) - nrow(chla_secchi) # Keep only the lakes with at least 200 observations of secchi and chla chla_secchi_200 &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% dplyr::mutate(count = n()) %&gt;% filter(count &gt; 200) 6.1.1.4 Join water quality data to spatial data spatial_200 &lt;- inner_join(spatial_lakes,chla_secchi_200 %&gt;% distinct(lagoslakeid,.keep_all=T), by=&#39;lagoslakeid&#39;) 6.1.2 Creating Map of Mean Chl_a ### Take the mean chl_a and secchi by lake mean_values_200 &lt;- chla_secchi_200 %&gt;% # Take summary by lake id dplyr::group_by(lagoslakeid) %&gt;% # take mean chl_a per lake id dplyr::summarize(mean_chl = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T)) %&gt;% #Get rid of NAs dplyr::filter(!is.na(mean_chl), !is.na(mean_secchi)) %&gt;% # Take the log base 10 of the mean_chl dplyr::mutate(log10_mean_chl = log10(mean_chl)) #Join datasets mean_spatial &lt;- inner_join(spatial_lakes,mean_values_200, by=&#39;lagoslakeid&#39;) #Make a map mapview(mean_spatial,zcol=&#39;log10_mean_chl&#39;, legend = TRUE, layer.name = &#39;Mean Chlorophyl a Concentration (g per L)&#39;) 6.2 Class work 6.2.0.1 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations? Here, I just want a plot of chla vs secchi for all sites # creating simple scatter plot ggplot2::ggplot(mean_values_200, aes(x = mean_secchi, y = mean_chl)) + geom_point() + ylab(&quot;Mean Chlorophyll a Concentration (g/L)&quot;)+ xlab(&quot;Secchi Disk Transparency (m)&quot;) The deeper the mean_secchi value, the lower the mean_chl value. This relationship appears logarithmic as well. 6.2.0.1.1 Why might this be the case? It makes sense that as nutrient loads in the water increases, the higher the chlorophyll a content will be, and the less-clear the water becomes. As the water becomes less clear, the depth at which the secchi disk disappears becomes shallower. 6.2.0.2 2) What states have the most data? The student assumes this means the most chla and secchi combined data, only. Therefore the student does not factor in the doc data. 6.2.0.2.1 2a) First you will need to make a lagos spatial dataset that has the total number of counts per site. chla_secchi_all &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi))%&gt;% #!is.na(doc))%&gt;% group_by(lagoslakeid) %&gt;% dplyr::mutate(COUNT = n()) %&gt;% arrange(desc(COUNT)) 6.2.0.2.2 2b) Second, you will need to join this point dataset to the us_boundaries data. ## Your code here all_states &lt;- us_states() ## Taking only lakes that are within the US States: us_lakes &lt;- spatial_lakes[all_states,] ## Joining us_lakes to chla_secchi_all ### Keeping all of the chla and secchi data and adding the lake centroid to each lagoslakeid observation: chla_secchi_all_us &lt;- left_join(chla_secchi_all,us_lakes, by = &#39;lagoslakeid&#39;, keep = FALSE) ## converting data to spatial data spatial_wq_us &lt;- st_as_sf(chla_secchi_all_us, crs = 4326) ## Joining spatial_chla_secchi_all to all_states by geometry chla_secchi_all_by_state &lt;- st_join(spatial_wq_us, all_states, join = st_intersects) 6.2.0.2.3 2c) Then you will want to group by state and sum all the observations in that state and arrange that data from most to least total observations per state. ## grouping, counting, and arranging by count: wq_by_state &lt;- chla_secchi_all_by_state %&gt;% group_by(state_abbr)%&gt;% dplyr::summarise(tot_obsv = n())%&gt;% arrange(-tot_obsv)%&gt;% filter(!is.na(state_abbr)) ## getting rid of geometry column cleaned_obs &lt;- st_set_geometry(wq_by_state, NULL) ## plotting observations for comparison: ggplot(cleaned_obs, aes(x = reorder(state_abbr,-tot_obsv),y = tot_obsv)) + geom_bar(stat=&quot;identity&quot;, width=0.5)+ xlab(&quot;State&quot;)+ ylab(&quot;Instances&quot;) + ggtitle(&quot;Instances of Combined Chlorophyll a and Secchi Data per State&quot;) As can be seen in the above bar chart, Minnesota has the highest number of instances of combined secchi and chlorophyll a data, followed by Wisconsin and then New York. 6.2.0.3 3) Is there a spatial pattern in Secchi disk depth for lakes with at least 200 observations? 6.2.0.4 Student assumes that this question is trying to ask, Looking at a map of mean Secchi disk depth values of lakes having more than 200 Secchi disk depth measurements, can we detect a spatial pattern in mean Secchi disk depth? (This assumption is based on lack of available unique geometry data for each individual secchi observation, and shear number of lakes having more than 200 Secchi observations) Also, student assumes that whether or not site has chla data is irrelevant for this question. ## filtering nutr down to only lakes with 200+ secchi observations, ... ## ...and selecting only lakeid, secchi, and observations columns... sdd_only &lt;- nutr %&gt;% select(lagoslakeid,secchi)%&gt;% filter(!is.na(secchi))%&gt;% group_by(lagoslakeid)%&gt;% dplyr::mutate(lake_obsv = n())%&gt;% filter(lake_obsv &gt; 200)%&gt;% arrange(-lake_obsv) ## getting mean secchi disk depths for each lake id sdd_means &lt;- sdd_only %&gt;% dplyr::group_by(lagoslakeid)%&gt;% dplyr::summarise(sdd_mean = mean(secchi)) ##cleaning spatial lakes down to lagoslakeid and geometry: sp_lakes_cln &lt;- spatial_lakes %&gt;% select(lagoslakeid) ##joining geometry onto the sdd means by lagoslakeid: sdd_means_geo &lt;- left_join(sdd_means,sp_lakes_cln, by = &#39;lagoslakeid&#39;, keep = FALSE) ## converting sdd_means_geo to spatial object: sdd_plot &lt;- st_as_sf(sdd_means_geo, crs=4326) ## mapping the mean secchi disk depth values for all lakes having over 200 observations of secchi disk depth. mapview(sdd_plot, zcol = &#39;sdd_mean&#39;, legend = TRUE, layer.name = &#39;Mean Secchi Disk Transarency (m)&#39;) As can be seen by the map of mean secchi disk depths for lakes having over 200 secchi observations, the disk depths tend to be deepest in the Northeast United States. The secchi disk depths tend to become shallower as the lake locations move further west. "],["regressions.html", "Chapter 7 Regressions 7.1 Weather Data Analysis 7.2 Assignment", " Chapter 7 Regressions Assignment 6: Using PRISM weather data and USDA NASS crop data to learn multiple regression 7.1 Weather Data Analysis 7.1.1 Load the PRISM daily maximum temperature # daily max temperature # dimensions of the data: counties x days x years prism &lt;- readMat(&quot;data_reg/prismiowa.mat&quot;) #look at all temp for county #1 in first year of data (1981): t_1981_c1 &lt;- prism$tmaxdaily.iowa[1,,1] t_1981_c1[366] #(shows that 1981 was not a leap year.) ## [1] NaN plot(1:366, t_1981_c1, type = &quot;l&quot;) ggplot()+ geom_line(mapping = aes(x=1:366, y=t_1981_c1))+ theme_bw()+ xlab(&quot;day of year&quot;) + ylab(&quot;daily maximum temperature (°C)&quot;)+ #use Alt+0176 as keyboard shortcut to get the degree symbol!!! ggtitle(&quot;Daily Maximum Temperature for County #1 in Iowa&quot;) # assign dimension names to tmax matrix dimnames(prism$tmaxdaily.iowa) &lt;- list(prism$COUNTYFP, 1:366, prism$years) # converting 3D matrxi to a dataframe: tmaxdf &lt;- as.data.frame.table(prism$tmaxdaily.iowa) # relabel the columns colnames(tmaxdf) &lt;- c(&quot;countyfp&quot;,&quot;doy&quot;,&quot;year&quot;,&quot;tmax&quot;) tmaxdf &lt;- tibble(tmaxdf) # tidying up 7.1.2 Temperature Trends 7.1.2.1 Summer Temperature Trends: Winneshiek County # converting some factors into dbl: tmaxdf$doy &lt;- as.numeric(tmaxdf$doy) tmaxdf$year &lt;- as.numeric(as.character(tmaxdf$year)) # creating variable for containing summer temp values of county 191 winnesummer &lt;- tmaxdf %&gt;% filter(countyfp == 191 &amp; doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% dplyr::group_by(year)%&gt;% dplyr::summarize(meantmax = mean(tmax)) ggplot(winnesummer, mapping = aes(x = year, y = meantmax))+ geom_point()+ theme_bw() + labs(x = &quot;year&quot;, y = &#39;Maximum Temperature (°C)&#39;)+ ggtitle(&quot;Average Daily Max Summer Temperature for Winneshiek County, Iowa&quot;) geom_smooth(method = lm) ## geom_smooth: na.rm = FALSE, orientation = NA, se = TRUE ## stat_smooth: na.rm = FALSE, orientation = NA, se = TRUE, method = function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) ## { ## ret.x &lt;- x ## ret.y &lt;- y ## cl &lt;- match.call() ## mf &lt;- match.call(expand.dots = FALSE) ## m &lt;- match(c(&quot;formula&quot;, &quot;data&quot;, &quot;subset&quot;, &quot;weights&quot;, &quot;na.action&quot;, &quot;offset&quot;), names(mf), 0) ## mf &lt;- mf[c(1, m)] ## mf$drop.unused.levels &lt;- TRUE ## mf[[1]] &lt;- quote(stats::model.frame) ## mf &lt;- eval(mf, parent.frame()) ## if (method == &quot;model.frame&quot;) ## return(mf) ## else if (method != &quot;qr&quot;) ## warning(gettextf(&quot;method = &#39;%s&#39; is not supported. Using &#39;qr&#39;&quot;, method), domain = NA) ## mt &lt;- attr(mf, &quot;terms&quot;) ## y &lt;- model.response(mf, &quot;numeric&quot;) ## w &lt;- as.vector(model.weights(mf)) ## if (!is.null(w) &amp;&amp; !is.numeric(w)) ## stop(&quot;&#39;weights&#39; must be a numeric vector&quot;) ## offset &lt;- model.offset(mf) ## mlm &lt;- is.matrix(y) ## ny &lt;- if (mlm) ## nrow(y) ## else length(y) ## if (!is.null(offset)) { ## if (!mlm) ## offset &lt;- as.vector(offset) ## if (NROW(offset) != ny) ## stop(gettextf(&quot;number of offsets is %d, should equal %d (number of observations)&quot;, NROW(offset), ny), domain = NA) ## } ## if (is.empty.model(mt)) { ## x &lt;- NULL ## z &lt;- list(coefficients = if (mlm) matrix(NA, 0, ncol(y)) else numeric(), residuals = y, fitted.values = 0 * y, weights = w, rank = 0, df.residual = if (!is.null(w)) sum(w != 0) else ny) ## if (!is.null(offset)) { ## z$fitted.values &lt;- offset ## z$residuals &lt;- y - offset ## } ## } ## else { ## x &lt;- model.matrix(mt, mf, contrasts) ## z &lt;- if (is.null(w)) ## lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) ## else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, ...) ## } ## class(z) &lt;- c(if (mlm) &quot;mlm&quot;, &quot;lm&quot;) ## z$na.action &lt;- attr(mf, &quot;na.action&quot;) ## z$offset &lt;- offset ## z$contrasts &lt;- attr(x, &quot;contrasts&quot;) ## z$xlevels &lt;- .getXlevels(mt, mf) ## z$call &lt;- cl ## z$terms &lt;- mt ## if (model) ## z$model &lt;- mf ## if (ret.x) ## z$x &lt;- x ## if (ret.y) ## z$y &lt;- y ## if (!qr) ## z$qr &lt;- NULL ## z ## } ## position_identity lm_summertmax &lt;- lm(meantmax ~ year, winnesummer) summary(lm_summertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnesummer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5189 -0.7867 -0.0341 0.6859 3.7415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.57670 36.44848 1.141 0.262 ## year -0.00747 0.01823 -0.410 0.684 ## ## Residual standard error: 1.232 on 36 degrees of freedom ## Multiple R-squared: 0.004644, Adjusted R-squared: -0.02301 ## F-statistic: 0.168 on 1 and 36 DF, p-value: 0.6844 7.1.2.2 Winter Temperatures trends: Winneshek County # creating variable for containing winter temp values of county 191 winnewinter &lt;- tmaxdf %&gt;% filter(countyfp == 191 &amp; (doy &lt;= 59 | doy &gt;= 335) &amp; !is.na(tmax)) %&gt;% dplyr::group_by(year)%&gt;% dplyr::summarize(meantmax = mean(tmax)) ggplot(winnewinter, mapping = aes(x = year, y = meantmax))+ geom_point()+ theme_bw() + labs(x = &quot;year&quot;, y = &quot;Average Daily Max Winter Temperature (°C)&quot;)+ ggtitle(&#39;Average Daily Max Temperature in Winter for Winneshiek County, Iowa&#39;)+ geom_smooth(method = lm) lm_wintertmax &lt;- lm(meantmax ~ year, winnewinter) summary(lm_wintertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5978 -1.4917 -0.3053 1.3778 4.5709 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.87825 60.48100 -0.494 0.624 ## year 0.01368 0.03025 0.452 0.654 ## ## Residual standard error: 2.045 on 36 degrees of freedom ## Multiple R-squared: 0.005652, Adjusted R-squared: -0.02197 ## F-statistic: 0.2046 on 1 and 36 DF, p-value: 0.6537 7.1.3 Multiple Regression  Quadratic Time Trend winnewinter$yearsq &lt;- winnewinter$year^2 lm_wintertmaxquad &lt;- lm(meantmax ~ year + yearsq, winnewinter) #summary(lm_winntertmaxquad) winnewinter$fitted &lt;- lm_wintertmaxquad$fitted.values ggplot(winnewinter)+ geom_point(mapping = aes(x = year, y = meantmax)) + geom_line(mapping = aes(x = year, y = fitted))+ theme_bw() + labs(x = &quot;year&quot;, y = &quot;Average Daily Max Winter Temperature (°C)&quot;)+ ggtitle(&quot;Average Max Daily Winter Temperature for Winneshiek County, Iowa&quot;) 7.1.4 Download NASS corn yield data ## set our API key with NASS nassqs_auth(key = &quot;6C0A99BB-3CB7-3E99-A699-B9DB7FDB8D16&quot;) ## parameters to query on params &lt;- list(commodity_desc = &quot;CORN&quot;, util_practice_desc = &quot;GRAIN&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) # download cornyieldsall &lt;- nassqs_yields(params) #convert to numeric cornyieldsall$county_ansi &lt;- as.numeric(cornyieldsall$county_ansi) cornyieldsall$yield &lt;- as.numeric(cornyieldsall$Value) cornyieldsall$year &lt;- as.numeric(cornyieldsall$year) #clean and filter dataset cornyields &lt;- select(cornyieldsall,county_ansi, county_name, yield, year)%&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) # make a tibble cornyields &lt;- tibble(cornyields) 7.2 Assignment 7.2.1 Question 1a: Extract Winneshiek County corn yields, fit a linear time trend, make a plot. Is there a significant time trend? # sifting out to have only the Winneshiek data wnnshk_uncln &lt;- cornyields %&gt;% filter(county_name == &quot;WINNESHIEK&quot;) #making sure that there aren&#39;t any rows with NAs: wnnshk &lt;- na.omit(wnnshk_uncln) # Fitting linear regression for time trend: lm_wnnshk &lt;- lm(yield ~ year, data = wnnshk) wnnshk$linear &lt;- lm_wnnshk$fitted.values summary(lm_wnnshk) ## ## Call: ## lm(formula = yield ~ year, data = wnnshk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.163 -1.841 2.363 9.437 24.376 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4763.290 448.286 -10.63 4.46e-13 *** ## year 2.457 0.224 10.96 1.77e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.97 on 39 degrees of freedom ## Multiple R-squared: 0.7551, Adjusted R-squared: 0.7488 ## F-statistic: 120.2 on 1 and 39 DF, p-value: 1.767e-13 According the summary statistics for the linear regression model fitted to the yield ~ year data for Winneshiek, there is indeed a quite significant time trend. The slope has a value of 2.457 yield units increase per year, and the p-value for that slope is 1.77e-13. # plotting yield ~ time along with linear regression: ggplot(wnnshk) + geom_point(mapping = aes(x = year, y = yield)) + geom_line(mapping = aes(x = year, y = linear)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;yield (bu/acre)&quot;)+ ggtitle(&quot;Corn Yield in Winneshiek County, Iowa&quot;) 7.2.2 Question 1b: Fit a quadratic time trend (i.e., year + year^2) and make a plot. Is there evidence for slowing yield growth? ## Fitting quadratic time trend wnnshk$yearsq &lt;- wnnshk$year^2 qd_wnnshk &lt;- lm(yield ~ (year + yearsq), data = wnnshk) wnnshk$quadratic &lt;- qd_wnnshk$fitted.values summary(qd_wnnshk) ## ## Call: ## lm(formula = yield ~ (year + yearsq), data = wnnshk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.384 -3.115 1.388 9.743 25.324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.583e+04 8.580e+04 0.301 0.765 ## year -2.812e+01 8.576e+01 -0.328 0.745 ## yearsq 7.641e-03 2.143e-02 0.357 0.723 ## ## Residual standard error: 17.17 on 38 degrees of freedom ## Multiple R-squared: 0.7559, Adjusted R-squared: 0.7431 ## F-statistic: 58.84 on 2 and 38 DF, p-value: 2.311e-12 ## plotting quadratic transform: ggplot(wnnshk) + geom_point(mapping = aes(x = year, y = yield)) + geom_smooth(mapping = aes(x = year, y = quadratic))+ labs(x = &quot;year&quot;, y = &quot;yield (bu/acre)&quot;)+ ggtitle(&#39;Corn Yield in Winneshiek County, Iowa&#39;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; In the summary statistics for the quadratic regression model, we see that the coefficient for yearsq is 7.641e-03, a positive value, but its p-value is large at 0.723. Due to this large p-value, we conclude that the quadratic regression model does not fit the data well. We do not have evidence that yields are either slowing or growing with each year. If the p-value were small (&lt; 0.05), we would conclude that yields are growing with each year, due to the fact that the yearsq coefficient is positive  meaning that the model is U-shaped rather than n-shaped  and that the data would clearly be the right-side of the U (as the plot makes obvious). 7.2.3 Question 2  Time Series: Lets analyze the relationship between temperature and yields for the Winneshiek County time series. Use data on yield and summer avg Tmax. Is adding year or Tmax^2 to your model helpful? Make a plot and interpret the results. # Joining the yield data to the temp data. ## (Note: temp data ends at 2018 and yield goes to 2021.) wnnshk_ty &lt;- left_join(winnesummer, wnnshk, by = &#39;year&#39;)%&gt;% filter(!is.na(yield), !is.na(meantmax)) 7.2.3.1 First Looking at the Linear Model for Yield ~ MeanTmax # Creating linear model for analyzing Yield ~ meanTmax: lm_w_ty &lt;- lm(yield ~ meantmax, data = wnnshk_ty) wnnshk_ty$lin_ty &lt;- lm_w_ty$fitted.values # summary stats for linear model summary(lm_w_ty) ## ## Call: ## lm(formula = yield ~ meantmax, data = wnnshk_ty) ## ## Residuals: ## Min 1Q Median 3Q Max ## -71.96 -19.85 -3.19 24.64 61.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 275.876 118.335 2.331 0.0255 * ## meantmax -4.763 4.438 -1.073 0.2902 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.88 on 36 degrees of freedom ## Multiple R-squared: 0.03101, Adjusted R-squared: 0.004098 ## F-statistic: 1.152 on 1 and 36 DF, p-value: 0.2902 Note: The summary statistics for the linear model prove that it is not a good fit for this data. The p-value for the coefficient of meantmax is 0.2902, which is much greater than 0.05. # plotting linear model ggplot(wnnshk_ty) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x = meantmax, y = lin_ty)) + theme_bw() + labs(x = &quot;Average Daily Maximum Summer Temperature (°C)&quot;, y = &quot;Corn Yield (bu/acre)&quot;)+ ggtitle(&quot;Corn Yield vs. Average Daily Max Summer Temperature&quot;, subtitle = &quot;Winneshiek County, Iowa&quot;) 7.2.3.2 Second, Examining the Quadratic Model for Yield ~ (MeanTmax^2 + MeanTmax) # Creating quadratic model for analyzing Yield ~ (Tmax^2 + Tmax) ## first creating meantmaxsq data: wnnshk_ty$meantmaxsq &lt;- wnnshk_ty$meantmax^2 ## now generating quadratic model: qd_w_ty &lt;- lm(yield ~ (meantmax + meantmaxsq), data = wnnshk_ty) ## pulling fitted values from quadratic model and adding to dataframe: wnnshk_ty$qd_ty &lt;- qd_w_ty$fitted.values ## looking at the quadratic model&#39;s summary statistics: summary(qd_w_ty) ## ## Call: ## lm(formula = yield ~ (meantmax + meantmaxsq), data = wnnshk_ty) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.587 -22.262 -0.982 22.409 52.798 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4223.604 1446.639 -2.920 0.00609 ** ## meantmax 328.918 107.068 3.072 0.00410 ** ## meantmaxsq -6.173 1.979 -3.119 0.00362 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.5 on 35 degrees of freedom ## Multiple R-squared: 0.2417, Adjusted R-squared: 0.1984 ## F-statistic: 5.579 on 2 and 35 DF, p-value: 0.007887 # plotting quadratic model ggplot(wnnshk_ty)+ geom_point(mapping = aes(x = meantmax, y = yield)) + geom_smooth(mapping = aes(x = meantmax, y = qd_ty))+ theme_bw()+ labs(x = &quot;Average Daily Max Summer Temperature (°C)&quot;, y = &quot;Corn Yield (bu/acre)&quot;)+ ggtitle(&quot;Corn Yield vs. Average Daily Max Summer Temperature&quot;, subtitle = &quot;Winneshiek County, Iowa&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The summary statistics for the quadratic regression model for Yield ~ (meantmax^2 + meantmax) shows that the quadratic model is indeed helpful for understanding the trend of the data. We can tell this by looking at the p-values for coefficients of meantmax, meantmaxsq, and the intercept, all of which are quite small and substantially lower than 0.05. The plot of this quadratic model, as shown above, clearly displays a negative parabolic curve (which regression models summary statistics also indicated). This type of relationship was to be expected, as optimum temperatures for maximizing yields must be neither too low nor too high. The quadratic model for this data should be accepted, while the linear model should be rejected. 7.2.4 Question 3  Cross-Section: Analyze the relationship between temperature and yield across all counties in 2018. Is there a relationship? Interpret the results. # Filtering down the temperature data to only what we need... t_18 &lt;- tmaxdf %&gt;% filter(year == 2018, (doy &gt; 151 &amp; doy &lt; 244)) %&gt;% # ...then grouping by county... dplyr::group_by(countyfp) %&gt;% # ...and calculating meanTmax for each county dplyr::summarise(meantmax = mean(tmax)) # going ahead and converting countyfp to numeric: t_18$countyfp &lt;- as.numeric(as.character(t_18$countyfp)) # Filtering down cornyields data to only 2018 cy_18 &lt;- cornyields %&gt;% filter(year == 2018, # making sure all NA&#39;s are gone !is.na(yield), !is.na(county_name),!is.na(county_ansi)) %&gt;% # creating countyfp column in preparation for joining mutate(countyfp = county_ansi) # Joining t_18 to cy_18, and selecting down to meantmax, yield, countyfp tcy_18 &lt;- left_join(cy_18,t_18, by = &quot;countyfp&quot;) %&gt;% select(countyfp, meantmax, yield) %&gt;% filter(!is.na(countyfp),!is.na(meantmax),!is.na(yield)) # Creating quadratic regression model ## calculating meantmaxsq values tcy_18$meantmaxsq &lt;- tcy_18$meantmax^2 ## creating quadratic model qd_tcy_18 &lt;- lm(yield ~ (meantmaxsq + meantmax), tcy_18) ## adding model&#39;s values to tcy_18 dataframe tcy_18$qd_yvalues &lt;- qd_tcy_18$fitted.values summary(qd_tcy_18) ## ## Call: ## lm(formula = yield ~ (meantmaxsq + meantmax), data = tcy_18) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.221 -15.399 5.007 14.541 30.879 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5501.602 1860.830 -2.957 0.00397 ** ## meantmaxsq -7.256 2.321 -3.126 0.00239 ** ## meantmax 406.789 131.493 3.094 0.00263 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.75 on 90 degrees of freedom ## Multiple R-squared: 0.1317, Adjusted R-squared: 0.1124 ## F-statistic: 6.827 on 2 and 90 DF, p-value: 0.001736 # Saving fitted values for plotting # Plotting Yield ~ MeanTmax for 2018 (all Iowa counties) ggplot(tcy_18) + geom_point(mapping = aes(x = meantmax, y = yield))+ geom_smooth(mapping = aes(x = meantmax, y = qd_yvalues))+ theme_bw() + labs(x = &quot;Average Daily Max Summer Temperature (°C)&quot;, y = &quot;Corn Yield (bu/acre)&quot;, title = &quot;Corn Yield vs. Average Daily Max Summer Temp by Iowa County&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We would expect that the relationship between mean summer maximum daily temperatures and corn yields across all Iowa counties for the year of 2018 would be very similar to the relationship between the same variables for Winneshiek County across all years (shown in the previous question). As such, we begin by fitting a quadratic model. In the summary statistics for the quadratic regression model for (corn yield) ~ (meantmax^2 + meantmax), shown above, we can see that p-values for all coefficients are substantially less than 0.05. This indicates that the quadratic model does indeed fit well to the data. Further, we see that this model indicates that corn yields can be expected to be highest when the average maximum daily temperature for the summer months is 28°C. 7.2.5 Question 4  Panel: One way to leverage multiple time series is to group all data into what is called a panel regression. Convert the county ID code (countyfp or county_ansi) into factor using as.factor, then include this variable in a regression using all counties yield and summer temperature data. How does the significance of your temperature coefficients (Tmax, Tmax^2) change? Make a plot comparing actual and fitted yields and interpret the results of your model. Students question: are you asking me to create a plot that has actual yields on the y-axis and fitted yield on the x-axis? Im not sure what youre asking. It seems to me that what we really want is model that helps predict corn yields across all counties ## getting the temperature data: ### Filtering down the temperature data to only what we need... t &lt;- tmaxdf %&gt;% filter(doy &gt; 151 &amp; doy &lt; 244) %&gt;% ### ...then grouping by county... dplyr::group_by(countyfp,year) %&gt;% ### ...and calculating meanTmax for each county... dplyr::summarise(meantmax = mean(tmax)) %&gt;% ### ...adding meantmaxsq in prep for quadratic regression model... mutate(meantmaxsq = meantmax^2) ## `summarise()` has grouped output by &#39;countyfp&#39;. You can override using the ## `.groups` argument. ## going ahead and converting countyfp to numeric: t$countyfp &lt;- as.numeric(as.character(t$countyfp)) # Getting the Yield Data cy &lt;- cornyields %&gt;% # making sure all NA&#39;s are gone filter(!is.na(yield), !is.na(county_name),!is.na(county_ansi)) %&gt;% # creating countyfp column in preparation for joining mutate(countyfp = county_ansi) # Joining Temperature Data to Yield Data by county and year tcy &lt;- inner_join(cy,t,by = c(&quot;countyfp&quot;, &quot;year&quot;)) ## converting countyfp to factor (as instructed) tcy$countyfp &lt;- as.factor(tcy$countyfp) ## cleaning out unneeded columns: tcy &lt;- select(tcy,-c(&quot;county_ansi&quot;, &quot;county_name&quot;)) ## reordering columns to proper positions: tcy &lt;- tcy[,c(3,2,1,4,5)] 7.2.5.1 First attempting the panel regression as instructed, without using the panel regression function # First creating lm() that does NOT factor in countyfp, but does factor both meantmax and meantmaxsq: panel_lm_NO_cfp &lt;- lm(yield ~ (meantmaxsq + meantmax), data = tcy) summary(panel_lm_NO_cfp) ## ## Call: ## lm(formula = yield ~ (meantmaxsq + meantmax), data = tcy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -113.151 -21.982 -2.794 24.860 82.006 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2098.6219 139.4315 -15.05 &lt;2e-16 *** ## meantmaxsq -3.1665 0.1769 -17.90 &lt;2e-16 *** ## meantmax 168.9590 9.9402 17.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31.55 on 3745 degrees of freedom ## Multiple R-squared: 0.1947, Adjusted R-squared: 0.1943 ## F-statistic: 452.8 on 2 and 3745 DF, p-value: &lt; 2.2e-16 # Creating lm() that factors in meantmax, meantmaxsq, AND countyfp panel_lm_W_cfp &lt;- lm(yield ~ (meantmaxsq + meantmax + countyfp), data = tcy) summary(panel_lm_W_cfp) ## ## Call: ## lm(formula = yield ~ (meantmaxsq + meantmax + countyfp), data = tcy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -95.088 -21.438 -3.026 23.570 77.941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2102.2327 139.2946 -15.092 &lt; 2e-16 *** ## meantmaxsq -3.1324 0.1758 -17.820 &lt; 2e-16 *** ## meantmax 167.9699 9.8950 16.975 &lt; 2e-16 *** ## countyfp3 -4.5577 7.0388 -0.648 0.517347 ## countyfp5 1.9797 7.0743 0.280 0.779612 ## countyfp7 -19.1369 7.0864 -2.701 0.006955 ** ## countyfp9 4.3024 7.0416 0.611 0.541240 ## countyfp11 6.2752 7.0459 0.891 0.373195 ## countyfp13 6.2028 7.0520 0.880 0.379147 ## countyfp15 14.3129 7.0414 2.033 0.042157 * ## countyfp17 10.2595 7.0568 1.454 0.146076 ## countyfp19 6.6549 7.0596 0.943 0.345912 ## countyfp21 7.5427 7.0503 1.070 0.284760 ## countyfp23 8.0045 7.0493 1.135 0.256242 ## countyfp25 9.3627 7.0468 1.329 0.184048 ## countyfp27 8.8039 7.0426 1.250 0.211348 ## countyfp29 6.0858 7.0389 0.865 0.387315 ## countyfp31 14.9921 7.0443 2.128 0.033381 * ## countyfp33 3.7400 7.0668 0.529 0.596677 ## countyfp35 12.8689 7.0455 1.827 0.067848 . ## countyfp37 1.3837 7.0714 0.196 0.844876 ## countyfp39 -24.9874 7.0863 -3.526 0.000427 *** ## countyfp41 5.5213 7.0521 0.783 0.433716 ## countyfp43 7.9986 7.0648 1.132 0.257632 ## countyfp45 9.7142 7.0455 1.379 0.168043 ## countyfp47 5.5829 7.0436 0.793 0.428054 ## countyfp49 10.5962 7.0391 1.505 0.132323 ## countyfp51 -14.6474 7.0887 -2.066 0.038872 * ## countyfp53 -16.7641 7.0865 -2.366 0.018051 * ## countyfp55 8.6421 7.0667 1.223 0.221435 ## countyfp57 10.8382 7.0391 1.540 0.123719 ## countyfp59 2.0837 7.0635 0.295 0.768011 ## countyfp61 9.1371 7.0690 1.293 0.196245 ## countyfp63 6.4115 7.0706 0.907 0.364579 ## countyfp65 6.5576 7.0722 0.927 0.353862 ## countyfp67 3.8270 7.0595 0.542 0.587775 ## countyfp69 10.2572 7.0536 1.454 0.145983 ## countyfp71 15.9700 7.0526 2.264 0.023608 * ## countyfp73 14.2209 7.0396 2.020 0.043445 * ## countyfp75 10.3933 7.0506 1.474 0.140543 ## countyfp77 3.0582 7.0392 0.434 0.663988 ## countyfp79 12.1993 7.0442 1.732 0.083392 . ## countyfp81 7.8892 7.0692 1.116 0.264496 ## countyfp83 13.0042 7.0463 1.846 0.065040 . ## countyfp85 7.0324 7.0390 0.999 0.317828 ## countyfp87 5.8050 7.0396 0.825 0.409641 ## countyfp89 0.9492 7.1084 0.134 0.893781 ## countyfp91 8.8848 7.0596 1.259 0.208279 ## countyfp93 10.8610 7.0456 1.542 0.123276 ## countyfp95 6.7101 7.0403 0.953 0.340600 ## countyfp97 -2.2777 7.0533 -0.323 0.746768 ## countyfp99 13.8935 7.0402 1.973 0.048519 * ## countyfp101 2.1063 7.0445 0.299 0.764958 ## countyfp103 3.8916 7.0404 0.553 0.580470 ## countyfp105 6.8541 7.0503 0.972 0.331025 ## countyfp107 2.4290 7.0390 0.345 0.730052 ## countyfp109 11.2195 7.0616 1.589 0.112188 ## countyfp111 3.2019 7.0439 0.455 0.649452 ## countyfp113 5.5167 7.0475 0.783 0.433799 ## countyfp115 8.0182 7.0400 1.139 0.254798 ## countyfp117 -23.7701 7.1360 -3.331 0.000874 *** ## countyfp119 8.4973 7.0446 1.206 0.227815 ## countyfp121 -2.7616 7.0390 -0.392 0.694843 ## countyfp123 8.2674 7.0389 1.175 0.240260 ## countyfp125 2.0919 7.0389 0.297 0.766339 ## countyfp127 13.1465 7.0469 1.866 0.062181 . ## countyfp129 9.7388 7.1425 1.364 0.172806 ## countyfp131 6.9486 7.0897 0.980 0.327099 ## countyfp133 1.2117 7.0395 0.172 0.863346 ## countyfp135 -16.6425 7.0863 -2.349 0.018900 * ## countyfp137 6.6056 7.0404 0.938 0.348183 ## countyfp139 8.2715 7.0389 1.175 0.240027 ## countyfp141 13.1679 7.0506 1.868 0.061894 . ## countyfp143 7.9469 7.0654 1.125 0.260765 ## countyfp145 0.4131 7.0406 0.059 0.953217 ## countyfp147 6.1297 7.0527 0.869 0.384835 ## countyfp149 6.7353 7.0404 0.957 0.338798 ## countyfp151 10.3788 7.0476 1.473 0.140925 ## countyfp153 14.0238 7.0389 1.992 0.046410 * ## countyfp155 10.4059 7.0864 1.468 0.142075 ## countyfp157 10.1606 7.0397 1.443 0.149014 ## countyfp159 -20.5978 7.0389 -2.926 0.003451 ** ## countyfp161 8.4110 7.0478 1.193 0.232778 ## countyfp163 15.5360 7.0426 2.206 0.027446 * ## countyfp165 6.9179 7.0417 0.982 0.325957 ## countyfp167 14.7911 7.0423 2.100 0.035770 * ## countyfp169 10.2395 7.0458 1.453 0.146233 ## countyfp171 8.7662 7.0459 1.244 0.213519 ## countyfp173 -14.5056 7.0868 -2.047 0.040744 * ## countyfp175 -12.5580 7.0863 -1.772 0.076452 . ## countyfp177 -2.8371 7.0524 -0.402 0.687493 ## countyfp179 -3.5740 7.0874 -0.504 0.614105 ## countyfp181 -2.0199 7.0389 -0.287 0.774154 ## countyfp183 10.8835 7.0394 1.546 0.122169 ## countyfp185 -22.9077 7.0863 -3.233 0.001237 ** ## countyfp187 13.1211 7.0472 1.862 0.062700 . ## countyfp189 7.5593 7.0757 1.068 0.285441 ## countyfp191 4.0826 7.0867 0.576 0.564584 ## countyfp193 2.3508 7.0395 0.334 0.738436 ## countyfp195 5.9103 7.0954 0.833 0.404908 ## countyfp197 10.4404 7.0518 1.481 0.138817 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30.68 on 3647 degrees of freedom ## Multiple R-squared: 0.2585, Adjusted R-squared: 0.2382 ## F-statistic: 12.71 on 100 and 3647 DF, p-value: &lt; 2.2e-16 ** Based on the results Im seeing, I believe that I must be making some kind of mistake, but Im not sure what that could be. I think there should be a difference in the p-values for the coefficients, but I dont see any** From examining the two different linear regression models, one factoring in countyfp and one not, that including the countyfp as a factor in the regression formula did not affect the significance of the temperature coefficients to any detectable degree. However, including countyfp did slightly alter the values of the coefficients. With the inclusion of countyfp as a factor in the linear regression, meantmax coefficient changed from -3.1665 to -3.1324 while meantmaxsq changed from 168.9590 to 167.9699. 7.2.5.2 Plotting Actual Yield vs. Fitted Yield # Trying to obtain fitted values from the panel regression: tcy$p_lm_fit2 &lt;- panel_lm_W_cfp$fitted.values # Plotting yield vs p_lm_fit2 ggplot(tcy) + geom_point(mapping = aes(x = p_lm_fit2, y = yield))+ theme_bw()+ labs(title = &quot;Corn Yield vs Panel Regression Fitted Value&quot;, subtitle = &quot;All Iowa Counties, 1981 - 2018&quot;, x = &quot;Panel Regression Fitted Value&quot;, y = &quot;Actual Corn Yield (bu/acre)&quot;) 7.2.5.3 This is the Student Trying to Leverage the Panel Regression Funciton plm() Instead 7.2.5.3.1 First Determining whether Fixed Effects or Random Effects Model is Appropriate # First testing the the pooled OLS model: plmtest(yield ~ (meantmaxsq + meantmax), data = tcy, effect = &quot;twoways&quot;, type = &quot;ghm&quot;) ## ## Lagrange Multiplier Test - two-ways effects (Gourieroux, Holly and ## Monfort) for unbalanced panels ## ## data: yield ~ (meantmaxsq + meantmax) ## chibarsq = 113671, df0 = 0.00, df1 = 1.00, df2 = 2.00, w0 = 0.25, w1 = ## 0.50, w2 = 0.25, p-value &lt; 2.2e-16 ## alternative hypothesis: significant effects The p-value for this GHM test is very small, and we therefore reject the null hypothesis that there are no significant effects. To be honest, I dont understand what this test is really testing. I just know that were supposed to run prior to running a panel regression, according to Columbia University: (https://blogs.cul.columbia.edu/spotlights/2018/11/16/introduction-to-r-plm-package-3/) #running the Hausman test to determine whether fixed or random effects model is best phtest(yield ~ (meantmaxsq + meantmax), data = tcy, model = c(&quot;within&quot;,&quot;random&quot;)) ## ## Hausman Test ## ## data: yield ~ (meantmaxsq + meantmax) ## chisq = 14.986, df = 2, p-value = 0.000557 ## alternative hypothesis: one model is inconsistent The p-value for this Hausman test is small, therefore we conclude that fixed effects model is best. 7.2.5.4 Continuing wiht Fixed Effects model # Performing fixed effects panel regression p_tcy = pdata.frame(tcy, index = c(&quot;countyfp&quot;, &quot;year&quot;)) pan_reg_fe &lt;- plm(yield ~ (meantmaxsq + meantmax), data = p_tcy, model = &quot;within&quot;) ### saving fitted values to p_tcy p_tcy$fe &lt;- pan_reg_fe$model$yield ### also getting the residuals p_tcy$residuals &lt;- pan_reg_fe$residuals ### TRYING FE - RESIDUALS p_tcy$fitted &lt;- p_tcy$fe - p_tcy$residuals summary(pan_reg_fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = yield ~ (meantmaxsq + meantmax), data = p_tcy, ## model = &quot;within&quot;) ## ## Unbalanced Panel: n = 99, T = 36-38, N = 3748 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -95.0881 -21.4381 -3.0255 23.5699 77.9414 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## meantmaxsq -3.13244 0.17578 -17.820 &lt; 2.2e-16 *** ## meantmax 167.96992 9.89502 16.975 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 4119000 ## Residual Sum of Squares: 3433100 ## R-Squared: 0.16652 ## Adj. R-Squared: 0.14366 ## F-statistic: 364.307 on 2 and 3647 DF, p-value: &lt; 2.22e-16 NOTE: In comparing the coefficient results on the panel regression that used the plm() function (immediately above) to that which used the lm() function (further above), we see that the coefficients of meantmax and meantmaxsq are essentially exactly the same. 7.2.5.5 Now using the lm() function instead # TRYING TO FIGURE OUT HOW TO PLOT &quot;actual and fitted yields and interpret the results of your model.&quot; ## WHAT IF I DO A REGRESSION ON TOP OF THE plm to get a smooth line? ### I think that this is like completing a regression among all of the fitted values. reg_panreg &lt;- lm(fitted ~ (meantmaxsq + meantmax), p_tcy) p_tcy$reg_panreg_fit &lt;- reg_panreg$fitted.values summary(reg_panreg) ## ## Call: ## lm(formula = fitted ~ (meantmaxsq + meantmax), data = p_tcy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.401 -2.232 2.084 5.351 15.635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2098.6219 39.2415 -53.48 &lt;2e-16 *** ## meantmaxsq -3.1665 0.0498 -63.59 &lt;2e-16 *** ## meantmax 168.9590 2.7976 60.40 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.88 on 3745 degrees of freedom ## Multiple R-squared: 0.7533, Adjusted R-squared: 0.7531 ## F-statistic: 5717 on 2 and 3745 DF, p-value: &lt; 2.2e-16 Interestingly enough, this regression of the panel regression produces the same coefficient values as the linear regression that did not factor in the countyfp. This is likely wrong. But if we take the intercept value from the regression that did not include countyfp as a factor, and use the coefficients from the panel regression that did include countyfp as a factor we get the following equation for yield as a function of meantmax: yield = -3.13244meantmaxsq + 167.96992meantmax -2098.621 It would be interesting to plot this function over the entire data. 7.2.5.6 Plotting Above Model Determined by Panel Regression Over All Data # Creating a column on tcy that is this function shown above: tcy$experimental_fit &lt;- (-3.13244*tcy$meantmaxsq) + (167.96992*tcy$meantmax) - 2098.621 # Plotting the data and the above model: ggplot(data = tcy)+ geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x = meantmax, y = experimental_fit), color = &quot;blue&quot;, size = 1.25)+ theme_bw() + labs(x = &quot;Average Daily Maximum Temperature for Summer Months (°C)&quot;, y = &quot;Annual Corn Yield (bu/acre)&quot;, title = &quot;Corn Yield vs. Ave. Daily Max. Summer Temp.&quot;, subtitle = &quot;All Iowa Counties, 1981 - 2018&quot;) 7.2.6 Question 5  Soybeans: Download NASS data on soybean yields and explore either a time series relationship for a given county, the cross-sectional relationship for a given year, or a panel across all counties and years. 7.2.6.1 Downloading Soybean Data ## set our API key with NASS nassqs_auth(key = &quot;6C0A99BB-3CB7-3E99-A699-B9DB7FDB8D16&quot;) ## parameters to query on params2 &lt;- list(commodity_desc = &quot;SOYBEANS&quot;, statisticcat_desc = &quot;PRODUCTION&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) # download sbyieldsall &lt;- nassqs_yields(params2) #convert to numeric sbyieldsall$county_ansi &lt;- as.numeric(sbyieldsall$county_ansi) sbyieldsall$yield &lt;- as.numeric(sbyieldsall$Value) #clean and filter dataset sbyields &lt;- select(sbyieldsall,county_ansi, county_name, yield, year)%&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) # make a tibble sbyields &lt;- tibble(cornyields) %&gt;% dplyr::group_by(county_name) %&gt;% dplyr::mutate(datapoints = n())%&gt;% arrange(-datapoints) 7.2.6.2 Looking at the Trend of Soybeans Over Time in Sioux County # sifting out to have only the Winneshiek data sx_uncln &lt;- sbyields %&gt;% filter(county_name == &quot;SIOUX&quot;) #making sure that there aren&#39;t any rows with NAs: sx &lt;- na.omit(sx_uncln) # Fitting linear regression for time trend: lm_sx &lt;- lm(yield ~ year, data = sx) sx$linear &lt;- lm_sx$fitted.values summary(lm_sx) ## ## Call: ## lm(formula = yield ~ year, data = sx) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.765 -8.374 3.122 12.106 20.091 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5413.9462 420.0761 -12.89 1.23e-15 *** ## year 2.7852 0.2099 13.27 4.86e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.9 on 39 degrees of freedom ## Multiple R-squared: 0.8186, Adjusted R-squared: 0.814 ## F-statistic: 176 on 1 and 39 DF, p-value: 4.863e-16 According the summary statistics for the linear regression model fitted to the yield ~ year data for Sioux County, there is indeed a quite significant time trend. The slope has a value of 2.7852 yield units increase per year, and the p-value for that slope is 4.86e-16. # plotting yield ~ time along with linear regression: ggplot(sx) + geom_point(mapping = aes(x = year, y = yield)) + geom_line(mapping = aes(x = year, y = linear)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Annual Soybean Yield (bu/acre)&quot;)+ ggtitle(&quot;Annual Soybean Yield Over Time for Sioux County, Iowa&quot;) 7.2.6.3 Examining whether rate of yield is slowing or decreasing. ## Fitting quadratic time trend sx$yearsq &lt;- sx$year^2 qd_sx &lt;- lm(yield ~ (year + yearsq), data = sx) sx$quadratic &lt;- qd_sx$fitted.values summary(qd_sx) ## ## Call: ## lm(formula = yield ~ (year + yearsq), data = sx) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.845 -8.544 3.730 11.856 19.769 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.239e+04 8.049e+04 -0.278 0.782 ## year 1.975e+01 8.045e+01 0.245 0.807 ## yearsq -4.239e-03 2.010e-02 -0.211 0.834 ## ## Residual standard error: 16.1 on 38 degrees of freedom ## Multiple R-squared: 0.8188, Adjusted R-squared: 0.8093 ## F-statistic: 85.88 on 2 and 38 DF, p-value: 8.002e-15 We do not find evidence that yield is either slowing or growing with each year. 7.2.7 Bonus #1: Find a package to make a county map of Iowa displaying some sort of information about yields or weather. Interpret your map. 7.2.8 Bonus #2: Challenge question - map trends in corn yields by county across Iowa. Interpret your map. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
